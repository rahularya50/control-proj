\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}

\usepackage{afterpage}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{verse}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{epstopdf}
\usepackage{circuitikz}
\usepackage[separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[toc,page]{appendix}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
\usepackage{titling}
\usepackage{siunitx}
\usepackage{physics}

\usepackage{setspace}
% \doublespacing
\usepackage{float}


\pgfplotsset{compat=1.14}

%  Special math symbols
%       floor, ceiling, angled brackets
%-----------------------------------------------------------------------
\newcommand{\floor}[1]{\left\lfloor #1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1\right\rceil}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\RE}{\mathbb{R}}        % real space
\newcommand{\ZZ}{\mathbb{Z}}        % integers
\newcommand{\NN}{\mathbb{N}}        % natural numbers
\newcommand{\eps}{{\varepsilon}}    % prettier epsilon
%-----------------------------------------------------------------------
%  Tighter lists
%-----------------------------------------------------------------------
\newenvironment{itemize*}% Tighter itemized list
  {\begin{itemize}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{itemize}}
\newenvironment{description*}% Tighter description list
  {\begin{description}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{description}}
\newenvironment{enumerate*}% Tighter enumerated list
  {\begin{enumerate}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{enumerate}}
%-----------------------------------------------------------------------
% Typing shortcuts
%-----------------------------------------------------------------------
\newcommand{\X}{\mathbb{X}}
\newcommand{\SG}{\mathbf{S}}
\newcommand{\GE}{\mathcal{G}}
\newcommand{\ST}{\,:\,}
\renewcommand{\tilde}[1]{\widetilde{#1}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\sq}{\square}
\newcommand{\half}[1]{\frac{#1}{2}}
\newcommand{\inv}[1]{\frac{1}{#1}}
\newcommand{\alg}{\textsf{SplitReduce}}
\newcommand{\sz}[1]{\sigma_{#1}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\softOmega}{\widetilde{\Omega}} 
\newcommand{\softO}{\widetilde{O}}
\newcommand{\OO}{O^*}  %or \widetilde{O}?

\newcommand{\dx}{\mathrm{d}x}
\newcommand{\dy}{\mathrm{d}y}
\newcommand{\dz}{\mathrm{d}z}
\newcommand{\dt}{\mathrm{d}t}
\newcommand{\du}{\mathrm{d}u}
\newcommand{\dtheta}{\mathrm{d}\theta}
\newcommand{\dq}{\mathrm{d}q}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\dV}{\mathrm{d}V}
\newcommand{\dL}{\mathrm{d}L}
\newcommand{\dA}{\mathrm{d}A}
\newcommand{\dH}{\mathrm{d}H}
\newcommand{\df}{\mathrm{d}f}
\newcommand{\dg}{\mathrm{d}g}
\newcommand{\dr}{\mathrm{d}r}
\newcommand{\dw}{\mathrm{d}w}
\newcommand{\dI}{\mathrm{d}I}

\newcommand*\len[1]{\overline{#1}}


\newcommand\note[1]{\marginpar{\textcolor{red}{#1}}}
\newcommand*{\tageq}{\refstepcounter{equation}\tag{\theequation}}

\newcommand*{\equals}{=}

\usepackage{fancyhdr}

\pgfplotscreateplotcyclelist{grayscale}{
    thick,white!10!black,mark=x,mark options=solid, dashed\\%
    thick,white!20!black,mark=o,mark options=solid\\%
}

\newcommand{\mat}[1]{\ensuremath{\begin{bmatrix}#1\end{bmatrix}}}
\newcommand{\cat}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\newcommand{\eqn}[1]{\begin{alignat*}{2}#1\end{alignat*}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand*{\thus}{&\implies\quad&}

\newcommand{\answer}[1]{\framebox{$\displaystyle #1 $}}

\begin{document}

Consider a simplification where we know the value of the maximum $\lambda$, all other eigenvalues can be treated as being close to $0$, and our random samples are one dimensional and mutually orthogonal. Let $\vec{x}[i]$ be our state after $i$ timesteps, let $\vec{u}[i]$ be the control input we apply after timestep $i$, and let $\vec{d}[i]$ be the unit vector in the direction of our measurement after timestep $i$.

Let $\vec{v}$ be the direction of the unit eigenvector corresponding to $\lambda$. Therefore, we can approximate $\vec{x}[i] = k[i]\vec{v}$, where $k[i]$ is a scalar quantity, since the component of $\vec{x}[i]$ in all other directions will quickly decay to $0$. We may write our transition equation as
\eqn{
    && \vec{x}[i + 1] &= \lambda \vec{x}[i] + \text{proj}_{\vec{v}}(\vec{u}[i]) \\
    \thus k[i + 1] &= \lambda k[i] + \vec{u}[i] \cdot \vec{v}.
}
Therefore,
\eqn{
    && k[i] &= \lambda k[i - 1] + \vec{u}[i - 1] \cdot \vec{v} \\
    &&&= \lambda^2 k[i-2] + (\lambda \vec{u}[i - 2] + \vec{u}[i - 1])\cdot \vec{v} \\
    &&&= \ldots \\
    &&&= \lambda^i k[0] + (\lambda^{i - 1} \vec{u}[0] + \lambda^{i - 2} \vec{u}[1] + \ldots + \lambda^0\vec{u}[i - 1])\cdot \vec{v}
}

Since both $\lambda$ and all the $\vec{u}[i]$ are known, we may define
\[
    \vec{w}[i] = \lambda^{i - 1} \vec{u}[0] + \lambda^{i - 2} \vec{u}[1] + \ldots + \lambda^0\vec{u}[i - 1],
\]
where $\vec{w}[i]$ is known.

Our scalar observations $y[i]$ after $i$ timesteps are
\eqn{
    && y[i] &= \vec{d}[i] \cdot \vec{x}[i] \\
    &&&= k[i] (\vec{d}[i]\cdot\vec{v}) \\
    &&&= (\lambda^i k[0] + \vec{w}[i] \cdot \vec{v})(\vec{d}[i] \cdot \vec{v}).
}
One goal could be to try and determine the projection of $\vec{v}$ in the subspace spanned by the $\vec{d}[i]$. Let the $\alpha_i$ be such that
\[
    \vec{v} = \alpha_0\vec{d}[0] + \alpha_1\vec{d}[1] + \ldots + \alpha_i\vec{d}[i],
\]
and let the component of $\vec{w}[i]$ along the direction $\vec{d}[j]$ be ${w}_j[i]$.

Therefore, we can rewrite our observations as
\[
    y[i] = \alpha_i(\lambda^i k[0] + \alpha_0{w}_0[i] + \alpha_1{w}_1[i] + \ldots + \alpha_i{w}_i[i]).
\]

Since up to timesteps $i - 1$, we obtained approximations for $\alpha_j$ for all $j < i$, we could solve this quadratic equation to determine $\alpha_i$ (though we'd require knowledge of $k[0]$). Then, maybe we could try to apply control to zero out $\vec{x}$ using this approximation for $\vec{v}$?

\section*{Problem Setups}
\subsection*{Known eigenvectors, scalar control, unknown eigenvalues}
\[
    \vec{x}[t + 1] = \mat{\lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots  & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n} \vec{x}[t] + \mat{1 \\ 1 \\ \vdots \\ 1}u(t).
\]
We make observations $\vec{x}[t] \cdot \vec{d}[t]$ where the $\vec{d}[t]$ are randomly chosen, and attempt to deduce the $\lambda_i$ and apply scalar control $u(t)$ to stabilize the system.

\subsection*{Unknown eigenvectors, scalar control, unknown eigenvalues}
\[
    \vec{x}[t + 1] = P\mat{\lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots  & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n}P^T \vec{x}[t] + \mat{1 \\ 0 \\ \vdots \\ 0}u(t).
\]
We make observations $\vec{x}[t] \cdot \vec{d}[t]$ where the $\vec{d}[t]$ are randomly chosen, and attempt to deduce $P$ and the $\lambda_i$ and apply scalar control $u(t)$ to stabilize the system. The eigenvector basis $P$ is orthonormal but unknown to us, and the $\lambda_i$ are in decreasing order i.e. $\lambda_1 \gg \lambda_2 \gg \ldots \gg \lambda_n$. Note that none of the $\lambda_i$ are guaranteed to be stable.

\subsection*{Known eigenvectors, vector control, unknown eigenvalues}
\[
    \vec{x}[t + 1] = \mat{\lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots  & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n} \vec{x}[t] + \vec{u}(t).
\]
We make observations $\vec{x}[t] \cdot \vec{d}[t]$ where the $\vec{d}[t]$ are randomly chosen, and attempt to deduce the $\lambda_i$ and apply vector control $\vec{u}(t)$ to stabilize the system. This case seems the easiest to understand?

\subsection*{Unknown eigenvectors, vector control, known eigenvalues}
\[
    \vec{x}[t + 1] = P\mat{\lambda_1 & 0 & \cdots & 0 \\ 0 & \lambda_2 & \cdots & 0 \\ \vdots  & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \lambda_n}P^T \vec{x}[t] + \vec{u}(t).
\]
We make observations $\vec{x}[t] \cdot \vec{d}[t]$ where the $\vec{d}[t]$ are randomly chosen, and attempt to deduce $P$ and apply scalar control $u(t)$ to stabilize the system. The eigenvector basis $P$ is orthonormal but unknown to us, and the $\lambda_i$ are in decreasing order i.e. $\lambda_1 \gg \lambda_2 \gg \ldots \gg \lambda_n$. Note that all the $\lambda_i$ but $\lambda_1$ are guaranteed to be stable. This is the problem we have been working with so far.

\end{document}
